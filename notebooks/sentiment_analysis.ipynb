# Install necessary libraries
!pip install nltk scikit-learn pandas
!pip install spacy
!python -m spacy download en_core_web_sm

# Step 2: Import necessary libraries
import spacy
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Load SpaCy model
nlp = spacy.load('en_core_web_sm')

# Dataset
data = {
    'text': [
        'I love this product!', 'This is the worst thing I have ever bought.', 'I am so happy with my purchase!',
        'Not worth the money.', 'Great value for the price.', 'Amazing quality!', 'Will definitely buy again.',
        'Very disappointed.', 'Highly recommend this!', 'I will never buy this again.', 'Fantastic experience!',
        'The worst customer service ever.', 'Absolutely love it!', 'It broke after a week.', 'Superb quality!',
        'Not happy with the purchase.', 'Perfect for my needs!', 'This was a complete waste of money.',
        'Very satisfied with the result.', 'I would not recommend this to anyone.', 'So easy to use!', 'Terrible quality.',
        'I am really impressed.', 'Could be better.', 'I’m so pleased with this product!', 'Really bad product.',
        'Very useful and practical.', 'Completely unsatisfied.', 'Exceeded my expectations!', 'The quality is awful.',
        'Good product but overpriced.', 'It’s amazing!', 'Waste of time and money.', 'I am in love with this!',
        'Don’t waste your money.', 'Will purchase again.', 'One of the worst things I’ve ever bought.',
        'So convenient and easy to use.', 'Really great value.', 'This is my favorite product!', 'I regret this purchase.',
        'Exceptional service!', 'It broke so quickly.', 'I would not buy this again.', 'Perfect for my family.',
        'Not durable at all.', 'The best I’ve ever used.', 'It’s okay, but could be better.', 'This is a must-have.',
        'Definitely worth the money.', 'This is a piece of junk.', 'Love it so much!', 'Horrible experience.',
        'Just what I needed!', 'It’s a scam!', 'I’m so happy with this.', 'I don’t recommend it.', 'Worth every penny.',
        'Horrible quality.', 'Excellent product!', 'I would definitely not recommend this.', 'Best purchase ever!',
        'Very disappointed with this product.', 'I am thrilled with it!', 'It didn’t work as expected.',
        'Great purchase!', 'This is just what I was looking for.', 'Good, but not great.', 'Amazing performance.',
        'It broke the first time I used it.', 'Love the design!', 'Not worth the price.', 'It’s okay.',
        'Just perfect for me!', 'Very poor quality.', 'Fantastic product!', 'Does not live up to the hype.',
        'A great addition to my collection.', 'Extremely bad purchase.', 'Very happy with this.', 'Horrible.',
        'I can’t stop using it!', 'Really disappointing.', 'Amazing features and performance!', 'This product is terrible.',
        'I absolutely love it!', 'It’s really bad.', 'Totally worth the cost.', 'Poor quality and workmanship.',
        'I use it every day!', 'Not as expected.', 'I recommend this to everyone.', 'Extremely overpriced.',
        'Best decision I ever made!', 'I am unhappy with the quality.', 'Perfect for my needs!', 'Awful.',
        'Highly recommend this product.', 'Wouldn’t buy this again.', 'Incredible product.', 'This is so bad.',
        'So glad I bought it!', 'Very low quality.', 'This product works wonderfully!', 'Would not recommend.',
        'Great buy!', 'Really bad experience.', 'Love it!', 'The product is defective.'
    ],
    'sentiment': [
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative',
        'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative'
    ]
}

# Preprocess the dataset
data['text'] = data['text'][:100]
data['sentiment'] = data['sentiment'][:100]

# Convert the 'sentiment' column to numeric labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(data['sentiment'])

# Create DataFrame
df = pd.DataFrame(data)

# Step 3: Split the dataset into train and test sets (80% train, 20% test)
X = df['text']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Preprocess the text using SpaCy
def preprocess_text_spacy(text):
    text = text.lower()
    doc = nlp(text)
    words = [token.lemma_ for token in doc if token.text.isalpha()]
    return ' '.join(words)

# Apply preprocessing
X_train_cleaned = X_train.apply(preprocess_text_spacy)
X_test_cleaned = X_test.apply(preprocess_text_spacy)

# Step 5: Feature Extraction using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train_cleaned).toarray()
X_test_tfidf = tfidf.transform(X_test_cleaned).toarray()

# ------------------------------------------
# 1. Support Vector Machine (SVM) Hyperparameter Tuning
# ------------------------------------------
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'class_weight': ['balanced', None]
}

svm_grid = GridSearchCV(SVC(), param_grid_svm, cv=3, n_jobs=-1)
svm_grid.fit(X_train_tfidf, y_train)
best_svm_model = svm_grid.best_estimator_

# Evaluate SVM model
y_pred_svm = best_svm_model.predict(X_test_tfidf)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# ------------------------------------------
# 2. Logistic Regression Hyperparameter Tuning
# ------------------------------------------
param_grid_logreg = {
    'C': [0.1, 1, 10],
    'solver': ['liblinear', 'saga'],
    'max_iter': [100, 1000]
}

logreg_grid = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=3, n_jobs=-1)
logreg_grid.fit(X_train_tfidf, y_train)
best_logreg_model = logreg_grid.best_estimator_

# Evaluate Logistic Regression model
y_pred_logreg = best_logreg_model.predict(X_test_tfidf)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logreg))

# ------------------------------------------
# 3. Random Forest Hyperparameter Tuning
# ------------------------------------------
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'class_weight': ['balanced', None]
}

rf_grid = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=3, n_jobs=-1)
rf_grid.fit(X_train_tfidf, y_train)
best_rf_model = rf_grid.best_estimator_

# Evaluate Random Forest model
y_pred_rf = best_rf_model.predict(X_test_tfidf)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

# ------------------------------------------
# 4. XGBoost Hyperparameter Tuning
# ------------------------------------------
param_grid_xgb = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'n_estimators': [50, 100],
    'objective': ['binary:logistic']
}

xgb_grid = GridSearchCV(xgb.XGBClassifier(), param_grid_xgb, cv=3, n_jobs=-1)
xgb_grid.fit(X_train_tfidf, y_train)
best_xgb_model = xgb_grid.best_estimator_

# Evaluate XGBoost model
y_pred_xgb = best_xgb_model.predict(X_test_tfidf)
print("XGBoost Accuracy:", accuracy_score(y_test, y_pred_xgb))

# Step 6: Save all the models for future use
import joblib

joblib.dump(best_svm_model, 'svm_model.pkl')
joblib.dump(best_logreg_model, 'logreg_model.pkl')
joblib.dump(best_rf_model, 'rf_model.pkl')
joblib.dump(best_xgb_model, 'xgb_model.pkl')
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')

print("Models saved successfully.")
