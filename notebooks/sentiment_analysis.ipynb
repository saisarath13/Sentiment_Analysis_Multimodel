{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Multiple Models\n",
    "\n",
    "This Jupyter notebook demonstrates sentiment analysis using various machine learning models, including Support Vector Machine (SVM), Logistic Regression, Random Forest, and XGBoost. The notebook also includes steps for hyperparameter tuning, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn pandas\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "# Example data load - replace with actual sentiment data\n",
    "data = pd.read_csv('sentiment_data.csv')  # Replace with your dataset path\n",
    "print(data.head())\n",
    "\n",
    "# Data preprocessing\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_punct])\n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "data['label'] = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "# Split data into train and test sets\n",
    "X = data['processed_text']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text to TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate baseline models\n",
    "\n",
    "# 1. Logistic Regression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# 2. Random Forest\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# 3. XGBoost\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train_tfidf, y_train)\n",
    "y_pred_xgb = xgb_model.predict(X_test_tfidf)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Tuning for Logistic Regression\n",
    "param_grid_logreg = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 1000]\n",
    "}\n",
    "\n",
    "logreg_grid = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=3, n_jobs=-1)\n",
    "logreg_grid.fit(X_train_tfidf, y_train)\n",
    "best_logreg_model = logreg_grid.best_estimator_\n",
    "\n",
    "# Evaluate Logistic Regression model\n",
    "y_pred_logreg = best_logreg_model.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_logreg))\n",
    "print(classification_report(y_test, y_pred_logreg))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 3. Random Forest Hyperparameter Tuning\n",
    "# ------------------------------------------\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "rf_grid = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=3, n_jobs=-1)\n",
    "rf_grid.fit(X_train_tfidf, y_train)\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "\n",
    "# Evaluate Random Forest model\n",
    "y_pred_rf = best_rf_model.predict(X_test_tfidf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "# ------------------------------------------\n",
    "# 4. XGBoost Hyperparameter Tuning\n",
    "# ------------------------------------------\n",
    "param_grid_xgb = {\n",
    "    'max_depth': [3, 5, 7],                # Try different max_depth to prevent overfitting\n",
    "    'learning_rate': [0.01, 0.05, 0.1],    # Lower learning rate to prevent overfitting\n",
    "    'n_estimators': [100, 200, 300],       # Try different numbers of trees\n",
    "    'subsample': [0.7, 0.8, 1.0],           # Randomly sample different portions of data for each tree\n",
    "    'colsample_bytree': [0.7, 0.8, 1.0],    # Sample features at each split\n",
    "    'scale_pos_weight': [20, 30, 40]        # Adjust class imbalance weight further\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb.XGBClassifier(), param_grid_xgb, cv=3, n_jobs=-1)\n",
    "xgb_grid.fit(X_train_tfidf, y_train)\n",
    "best_xgb_model = xgb_grid.best_estimator_\n",
    "\n",
    "# Evaluate XGBoost model\n",
    "y_pred_xgb = best_xgb_model.predict(X_test_tfidf)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(classification_report(y_test, y_pred_xgb))"
   ]
  }
 ]
}
