!pip install nltk scikit-learn pandas
!pip install spacy
!python -m spacy download en_core_web_sm

# Step 2: Import necessary libraries
import spacy
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier

# Load SpaCy model
nlp = spacy.load('en_core_web_sm')

# Dataset
data = {
    'text': [
        'I love this product!', 'This is the worst thing I have ever bought.', 'I am so happy with my purchase!',
        'Not worth the money.', 'Great value for the price.', 'Amazing quality!', 'Will definitely buy again.',
        'Very disappointed.', 'Highly recommend this!', 'I will never buy this again.', 'Fantastic experience!',
        'The worst customer service ever.', 'Absolutely love it!', 'It broke after a week.', 'Superb quality!',
        'Not happy with the purchase.', 'Perfect for my needs!', 'This was a complete waste of money.',
        'Very satisfied with the result.', 'I would not recommend this to anyone.', 'So easy to use!', 'Terrible quality.',
        'I am really impressed.', 'Could be better.', 'I’m so pleased with this product!', 'Really bad product.',
        'Very useful and practical.', 'Completely unsatisfied.', 'Exceeded my expectations!', 'The quality is awful.',
        'Good product but overpriced.', 'It’s amazing!', 'Waste of time and money.', 'I am in love with this!',
        'Don’t waste your money.', 'Will purchase again.', 'One of the worst things I’ve ever bought.',
        'So convenient and easy to use.', 'Really great value.', 'This is my favorite product!', 'I regret this purchase.',
        'Exceptional service!', 'It broke so quickly.', 'I would not buy this again.', 'Perfect for my family.',
        'Not durable at all.', 'The best I’ve ever used.', 'It’s okay, but could be better.', 'This is a must-have.',
        'Definitely worth the money.', 'This is a piece of junk.', 'Love it so much!', 'Horrible experience.',
        'Just what I needed!', 'It’s a scam!', 'I’m so happy with this.', 'I don’t recommend it.', 'Worth every penny.',
        'Horrible quality.', 'Excellent product!', 'I would definitely not recommend this.', 'Best purchase ever!',
        'Very disappointed with this product.', 'I am thrilled with it!', 'It didn’t work as expected.',
        'Great purchase!', 'This is just what I was looking for.', 'Good, but not great.', 'Amazing performance.',
        'It broke the first time I used it.', 'Love the design!', 'Not worth the price.', 'It’s okay.',
        'Just perfect for me!', 'Very poor quality.', 'Fantastic product!', 'Does not live up to the hype.',
        'A great addition to my collection.', 'Extremely bad purchase.', 'Very happy with this.', 'Horrible.',
        'I can’t stop using it!', 'Really disappointing.', 'Amazing features and performance!', 'This product is terrible.',
        'I absolutely love it!', 'It’s really bad.', 'Totally worth the cost.', 'Poor quality and workmanship.',
        'I use it every day!', 'Not as expected.', 'I recommend this to everyone.', 'Extremely overpriced.',
        'Best decision I ever made!', 'I am unhappy with the quality.', 'Perfect for my needs!', 'Awful.',
        'Highly recommend this product.', 'Wouldn’t buy this again.', 'Incredible product.', 'This is so bad.',
        'So glad I bought it!', 'Very low quality.', 'This product works wonderfully!', 'Would not recommend.',
        'Great buy!', 'Really bad experience.', 'Love it!', 'The product is defective.'
    ],
    'sentiment': [
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative',
        'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',
        'positive', 'negative', 'positive', 'negative'
    ]
}

# Preprocess the dataset
data['text'] = data['text'][:100]
data['sentiment'] = data['sentiment'][:100]

# Convert the 'sentiment' column to numeric labels
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(data['sentiment'])

# Create DataFrame
df = pd.DataFrame(data)

# Step 3: Split the dataset into train and test sets (80% train, 20% test)
X = df['text']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Preprocess the text using SpaCy
def preprocess_text_spacy(text):
    text = text.lower()
    doc = nlp(text)
    words = [token.lemma_ for token in doc if token.text.isalpha()]
    return ' '.join(words)

# Apply preprocessing
X_train_cleaned = X_train.apply(preprocess_text_spacy)
X_test_cleaned = X_test.apply(preprocess_text_spacy)

# Step 5: Feature Extraction using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_train_tfidf = tfidf.fit_transform(X_train_cleaned).toarray()
X_test_tfidf = tfidf.transform(X_test_cleaned).toarray()

# ------------------------------------------
# 1. Support Vector Machine (SVM) Hyperparameter Tuning
# ------------------------------------------
param_grid_svm = {
    'C': [0.1, 1, 10],
    'kernel': ['linear', 'rbf'],
    'class_weight': ['balanced', None]
}

svm_grid = GridSearchCV(SVC(), param_grid_svm, cv=3, n_jobs=-1)
svm_grid.fit(X_train_tfidf, y_train)
best_svm_model = svm_grid.best_estimator_

# Evaluate SVM model
y_pred_svm = best_svm_model.predict(X_test_tfidf)
print("SVM Accuracy:", accuracy_score(y_test, y_pred_svm))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_svm))
print("Classification Report:\n", classification_report(y_test, y_pred_svm))

# ------------------------------------------
# 2. Logistic Regression Hyperparameter Tuning
# ------------------------------------------
param_grid_logreg = {
    'C': [0.1, 1, 10],
    'solver': ['liblinear', 'saga'],
    'max_iter': [100, 1000]
}

logreg_grid = GridSearchCV(LogisticRegression(), param_grid_logreg, cv=3, n_jobs=-1)
logreg_grid.fit(X_train_tfidf, y_train)
best_logreg_model = logreg_grid.best_estimator_

# Evaluate Logistic Regression model
y_pred_logreg = best_logreg_model.predict(X_test_tfidf)
print("Logistic Regression Accuracy:", accuracy_score(y_test, y_pred_logreg))

# ------------------------------------------
# 3. Random Forest Hyperparameter Tuning
# ------------------------------------------
param_grid_rf = {
    'n_estimators': [50, 100, 200],
    'max_depth': [10, 20, None],
    'class_weight': ['balanced', None]
}

rf_grid = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=3, n_jobs=-1)
rf_grid.fit(X_train_tfidf, y_train)
best_rf_model = rf_grid.best_estimator_

# Evaluate Random Forest model
y_pred_rf = best_rf_model.predict(X_test_tfidf)
print("Random Forest Accuracy:", accuracy_score(y_test, y_pred_rf))

import xgboost as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

# Updated XGBoost Hyperparameters for better handling of imbalance
param_grid_xgb = {
    'max_depth': [3, 5, 7],                # Try different max_depth to prevent overfitting
    'learning_rate': [0.01, 0.05, 0.1],    # Lower learning rate to prevent overfitting
    'n_estimators': [100, 200, 300],       # Try different numbers of trees
    'subsample': [0.7, 0.8, 1.0],           # Randomly sample different portions of data for each tree
    'colsample_bytree': [0.7, 0.8, 1.0],    # Sample features at each split
    'scale_pos_weight': [20, 30, 40]        # Adjust class imbalance weight further
}

# Initialize XGBoost model
xgb_model = xgb.XGBClassifier(use_label_encoder=False)

# Set up GridSearchCV with cross-validation
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=3, n_jobs=-1, verbose=2)

# Fit the model using GridSearchCV
grid_search_xgb.fit(X_train_tfidf, y_train)

# Best hyperparameters from GridSearchCV
best_xgb_model = grid_search_xgb.best_estimator_

# Predict on test data
y_pred_xgb = best_xgb_model.predict(X_test_tfidf)

# Evaluate the model performance
accuracy = accuracy_score(y_test, y_pred_xgb)
print("XGBoost Accuracy:", accuracy)
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb))
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))



import joblib

# Save the trained models and TF-IDF vectorizer
joblib.dump(best_svm_model, 'svm_model.pkl')
joblib.dump(best_logreg_model, 'logreg_model.pkl')
joblib.dump(best_rf_model, 'rf_model.pkl')
joblib.dump(best_xgb_model, 'xgb_model.pkl')
joblib.dump(tfidf, 'tfidf_vectorizer.pkl')


import joblib
import spacy
import pandas as pd

# Load the trained models and TF-IDF vectorizer
svm_model = joblib.load('svm_model.pkl')
logreg_model = joblib.load('logreg_model.pkl')
rf_model = joblib.load('rf_model.pkl')
xgb_model = joblib.load('xgb_model.pkl')
tfidf = joblib.load('tfidf_vectorizer.pkl')

# Load SpaCy model for preprocessing
nlp = spacy.load('en_core_web_sm')


def preprocess_text_spacy(text):
    text = text.lower()
    doc = nlp(text)
    words = [token.lemma_ for token in doc if token.text.isalpha()]
    return ' '.join(words)


def predict_sentiment(text):
    # Preprocess the text
    cleaned_text = preprocess_text_spacy(text)

    # Convert the cleaned text into TF-IDF features
    tfidf_features = tfidf.transform([cleaned_text]).toarray()

    # Get predictions from all models
    svm_pred = svm_model.predict(tfidf_features)
    logreg_pred = logreg_model.predict(tfidf_features)
    rf_pred = rf_model.predict(tfidf_features)
    xgb_pred = xgb_model.predict(tfidf_features)

    # Convert predictions to human-readable labels
    labels = ['negative', 'positive']

    # Return predictions
    return {
        'svm_prediction': labels[svm_pred[0]],
        'logreg_prediction': labels[logreg_pred[0]],
        'rf_prediction': labels[rf_pred[0]],
        'xgb_prediction': labels[xgb_pred[0]]
    }


text_input = "I really enjoy using this product, it's fantastic!"
predictions = predict_sentiment(text_input)
print(predictions)


import pickle

# Save the models using pickle
with open('models/svm_model.pkl', 'wb') as f:
    pickle.dump(svm_model, f)

with open('models/logreg_model.pkl', 'wb') as f:
    pickle.dump(logreg_model, f)

# Similarly for other models
with open('models/rf_model.pkl', 'wb') as f:
    pickle.dump(rf_model, f)

with open('models/xgb_model.pkl', 'wb') as f:
    pickle.dump(xgb_model, f)

with open('models/tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(tfidf, f)


import pickle

# Load the saved models using pickle
with open('models/svm_model.pkl', 'rb') as f:
    svm_model = pickle.load(f)

with open('models/logreg_model.pkl', 'rb') as f:
    logreg_model = pickle.load(f)

with open('models/rf_model.pkl', 'rb') as f:
    rf_model = pickle.load(f)

with open('models/xgb_model.pkl', 'rb') as f:
    xgb_model = pickle.load(f)

with open('models/tfidf_vectorizer.pkl', 'rb') as f:
    tfidf = pickle.load(f)
