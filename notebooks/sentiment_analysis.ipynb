{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Multiple Models\n",
    "\n",
    "This Jupyter notebook demonstrates sentiment analysis using various machine learning models, including Support Vector Machine (SVM), Logistic Regression, Random Forest, and XGBoost. The notebook also includes steps for hyperparameter tuning, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn pandas\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define Dataset #Import dataste file\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love this product!', 'This is the worst thing I have ever bought.', 'I am so happy with my purchase!',\n",
    "        'Not worth the money.', 'Great value for the price.', 'Amazing quality!', 'Will definitely buy again.',\n",
    "        'Very disappointed.', 'Highly recommend this!', 'I will never buy this again.', 'Fantastic experience!',\n",
    "        'The worst customer service ever.', 'Absolutely love it!', 'It broke after a week.', 'Superb quality!',\n",
    "        'Not happy with the purchase.', 'Perfect for my needs!', 'This was a complete waste of money.',\n",
    "        'Very satisfied with the result.', 'I would not recommend this to anyone.', 'So easy to use!', 'Terrible quality.',\n",
    "        'I am really impressed.', 'Could be better.', 'I’m so pleased with this product!', 'Really bad product.',\n",
    "        'Very useful and practical.', 'Completely unsatisfied.', 'Exceeded my expectations!', 'The quality is awful.',\n",
    "        'Good product but overpriced.', 'It’s amazing!', 'Waste of time and money.', 'I am in love with this!',\n",
    "        'Don’t waste your money.', 'Will purchase again.', 'One of the worst things I’ve ever bought.',\n",
    "        'So convenient and easy to use.', 'Really great value.', 'This is my favorite product!'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Preprocess the dataset\n",
    "data['text'] = data['text'][:100]\n",
    "data['sentiment'] = data['sentiment'][:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'sentiment' column to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the dataset into train and test sets (80% train, 20% test)\n",
    "X = df['text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Preprocess the text using SpaCy\n",
    "def preprocess_text_spacy(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_ for token in doc if token.text.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_cleaned = X_train.apply(preprocess_text_spacy)\n",
    "X_test_cleaned = X_test.apply(preprocess_text_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Feature Extraction using TF-IDF\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_cleaned).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test_cleaned).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 1. Support Vector Machine (SVM) Hyperparameter Tuning\n",
    "# ------------------------------------------\n",
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'class_weight': ['balanced', None]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(SVC(), param_grid_svm, cv=3, n_jobs=-1)\n",
    "svm_grid.fit(X_train_tfidf, y_train)\n",
    "best_svm_model = svm_grid.best_estimator_\n",
    "\n",
    "# Evaluate SVM model\n",
    "y_pred_svm = best_svm_model.predict(X_test_tfidf)\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_svm))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 2. Logistic Regression\n",
    "# ------------------------------------------\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "y_pred_lr = log_reg.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_score(y_test, y_pred_lr))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 3. Random Forest Classifier\n",
    "# ------------------------------------------\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred_rf = rf.predict(X_test_tfidf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# 4. XGBoost Classifier\n",
    "# ------------------------------------------\n",
    "xgb_clf = xgb.XGBClassifier()\n",
    "xgb_clf.fit(X_train_tfidf, y_train)\n",
    "y_pred_xgb = xgb_clf.predict(X_test_tfidf)\n",
    "print(\"XGBoost Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models using joblib\n",
    "joblib.dump(best_svm_model, 'svm_model.pkl')\n",
    "joblib.dump(log_reg, 'logistic_regression_model.pkl')\n",
    "joblib.dump(rf, 'random_forest_model.pkl')\n",
    "joblib.dump(xgb_clf, 'xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "This notebook demonstrates multiple machine learning models for sentiment analysis, including hyperparameter tuning and model evaluation. The results from each classifier were evaluated based on accuracy, confusion matrix, and classification report. The trained models are also saved for future use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
