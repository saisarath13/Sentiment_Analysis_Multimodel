{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b01a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn pandas\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f0391bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e987ad67",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'text': [\n",
    "        'I love this product!', 'This is the worst thing I have ever bought.', 'I am so happy with my purchase!',\n",
    "        'Not worth the money.', 'Great value for the price.', 'Amazing quality!', 'Will definitely buy again.',\n",
    "        'Very disappointed.', 'Highly recommend this!', 'I will never buy this again.', 'Fantastic experience!',\n",
    "        'The worst customer service ever.', 'Absolutely love it!', 'It broke after a week.', 'Superb quality!',\n",
    "        'Not happy with the purchase.', 'Perfect for my needs!', 'This was a complete waste of money.',\n",
    "        'Very satisfied with the result.', 'I would not recommend this to anyone.', 'So easy to use!', 'Terrible quality.',\n",
    "        'I am really impressed.', 'Could be better.', 'I’m so pleased with this product!', 'Really bad product.',\n",
    "        'Very useful and practical.', 'Completely unsatisfied.', 'Exceeded my expectations!', 'The quality is awful.',\n",
    "        'Good product but overpriced.', 'It’s amazing!', 'Waste of time and money.', 'I am in love with this!',\n",
    "        'Don’t waste your money.', 'Will purchase again.', 'One of the worst things I’ve ever bought.',\n",
    "        'So convenient and easy to use.', 'Really great value.', 'This is my favorite product!', 'I regret this purchase.',\n",
    "        'Exceptional service!', 'It broke so quickly.', 'I would not buy this again.', 'Perfect for my family.',\n",
    "        'Not durable at all.', 'The best I’ve ever used.', 'It’s okay, but could be better.', 'This is a must-have.',\n",
    "        'Definitely worth the money.', 'This is a piece of junk.', 'Love it so much!', 'Horrible experience.',\n",
    "        'Just what I needed!', 'It’s a scam!', 'I’m so happy with this.', 'I don’t recommend it.', 'Worth every penny.',\n",
    "        'Horrible quality.', 'Excellent product!', 'I would definitely not recommend this.', 'Best purchase ever!',\n",
    "        'Very disappointed with this product.', 'I am thrilled with it!', 'It didn’t work as expected.',\n",
    "        'Great purchase!', 'This is just what I was looking for.', 'Good, but not great.', 'Amazing performance.',\n",
    "        'It broke the first time I used it.', 'Love the design!', 'Not worth the price.', 'It’s okay.',\n",
    "        'Just perfect for me!', 'Very poor quality.', 'Fantastic product!', 'Does not live up to the hype.',\n",
    "        'A great addition to my collection.', 'Extremely bad purchase.', 'Very happy with this.', 'Horrible.',\n",
    "        'I can’t stop using it!', 'Really disappointing.', 'Amazing features and performance!', 'This product is terrible.',\n",
    "        'I absolutely love it!', 'It’s really bad.', 'Totally worth the cost.', 'Poor quality and workmanship.',\n",
    "        'I use it every day!', 'Not as expected.', 'I recommend this to everyone.', 'Extremely overpriced.',\n",
    "        'Best decision I ever made!', 'I am unhappy with the quality.', 'Perfect for my needs!', 'Awful.',\n",
    "        'Highly recommend this product.', 'Wouldn’t buy this again.', 'Incredible product.', 'This is so bad.',\n",
    "        'So glad I bought it!', 'Very low quality.', 'This product works wonderfully!', 'Would not recommend.',\n",
    "        'Great buy!', 'Really bad experience.', 'Love it!', 'The product is defective.'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative'\n",
    "    ]\n",
    "}\n",
    "data['text'] = data['text'][:100]\n",
    "data['sentiment'] = data['sentiment'][:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f149f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['sentiment'])\n",
    "df = pd.DataFrame(data)\n",
    "X = df['text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "def preprocess_text_spacy(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_ for token in doc if token.text.isalpha()]\n",
    "    return ' '.join(words)\n",
    "X_train_cleaned = X_train.apply(preprocess_text_spacy)\n",
    "X_test_cleaned = X_test.apply(preprocess_text_spacy)\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_cleaned).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test_cleaned).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc2e8f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_svm = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma': ['scale', 'auto']\n",
    "}\n",
    "grid_search_svm = GridSearchCV(SVC(), param_grid_svm, cv=5)\n",
    "grid_search_svm.fit(X_train_tfidf, y_train)\n",
    "best_params_svm = grid_search_svm.best_params_\n",
    "y_pred_svm = grid_search_svm.predict(X_test_tfidf)\n",
    "print('SVM Model Accuracy:', accuracy_score(y_test, y_pred_svm))\n",
    "print('SVM Confusion Matrix:', confusion_matrix(y_test, y_pred_svm))\n",
    "print('SVM Classification Report:', classification_report(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a581b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\n",
    "grid_search_rf.fit(X_train_tfidf, y_train)\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "y_pred_rf = grid_search_rf.predict(X_test_tfidf)\n",
    "print('Random Forest Model Accuracy:', accuracy_score(y_test, y_pred_rf))\n",
    "print('Random Forest Confusion Matrix:', confusion_matrix(y_test, y_pred_rf))\n",
    "print('Random Forest Classification Report:', classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09c93a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_lr = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga']\n",
    "}\n",
    "grid_search_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5)\n",
    "grid_search_lr.fit(X_train_tfidf, y_train)\n",
    "best_params_lr = grid_search_lr.best_params_\n",
    "y_pred_lr = grid_search_lr.predict(X_test_tfidf)\n",
    "print('Logistic Regression Model Accuracy:', accuracy_score(y_test, y_pred_lr))\n",
    "print('Logistic Regression Confusion Matrix:', confusion_matrix(y_test, y_pred_lr))\n",
    "print('Logistic Regression Classification Report:', classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d49530bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_xgb = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 6, 10]\n",
    "}\n",
    "grid_search_xgb = GridSearchCV(xgb.XGBClassifier(), param_grid_xgb, cv=5)\n",
    "grid_search_xgb.fit(X_train_tfidf, y_train)\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "y_pred_xgb = grid_search_xgb.predict(X_test_tfidf)\n",
    "print('XGBoost Model Accuracy:', accuracy_score(y_test, y_pred_xgb))\n",
    "print('XGBoost Confusion Matrix:', confusion_matrix(y_test, y_pred_xgb))\n",
    "print('XGBoost Classification Report:', classification_report(y_test, y_pred_xgb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
