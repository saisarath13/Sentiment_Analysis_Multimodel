{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with Multiple Models\n",
    "\n",
    "This Jupyter notebook demonstrates sentiment analysis using various machine learning models, including Support Vector Machine (SVM), Logistic Regression, Random Forest, and XGBoost. The notebook also includes steps for hyperparameter tuning, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk scikit-learn pandas\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "data = {\n",
    "    'text': [\n",
    "        'I love this product!', 'This is the worst thing I have ever bought.', 'I am so happy with my purchase!',\n",
    "        'Not worth the money.', 'Great value for the price.', 'Amazing quality!', 'Will definitely buy again.',\n",
    "        'Very disappointed.', 'Highly recommend this!', 'I will never buy this again.', 'Fantastic experience!',\n",
    "        'The worst customer service ever.', 'Absolutely love it!', 'It broke after a week.', 'Superb quality!',\n",
    "        'Not happy with the purchase.', 'Perfect for my needs!', 'This was a complete waste of money.',\n",
    "        'Very satisfied with the result.', 'I would not recommend this to anyone.', 'So easy to use!', 'Terrible quality.',\n",
    "        'I am really impressed.', 'Could be better.', 'I’m so pleased with this product!', 'Really bad product.',\n",
    "        'Very useful and practical.', 'Completely unsatisfied.', 'Exceeded my expectations!', 'The quality is awful.',\n",
    "        'Good product but overpriced.', 'It’s amazing!', 'Waste of time and money.', 'I am in love with this!',\n",
    "        'Don’t waste your money.', 'Will purchase again.', 'One of the worst things I’ve ever bought.',\n",
    "        'So convenient and easy to use.', 'Really great value.', 'This is my favorite product!', 'I regret this purchase.',\n",
    "        'Exceptional service!', 'It broke so quickly.', 'I would not buy this again.', 'Perfect for my family.',\n",
    "        'Not durable at all.', 'The best I’ve ever used.', 'It’s okay, but could be better.', 'This is a must-have.',\n",
    "        'Definitely worth the money.', 'This is a piece of junk.', 'Love it so much!', 'Horrible experience.',\n",
    "        'Just what I needed!', 'It’s a scam!', 'I’m so happy with this.', 'I don’t recommend it.', 'Worth every penny.',\n",
    "        'Horrible quality.', 'Excellent product!', 'I would definitely not recommend this.', 'Best purchase ever!',\n",
    "        'Very disappointed with this product.', 'I am thrilled with it!', 'It didn’t work as expected.',\n",
    "        'Great purchase!', 'This is just what I was looking for.', 'Good, but not great.', 'Amazing performance.',\n",
    "        'It broke the first time I used it.', 'Love the design!', 'Not worth the price.', 'It’s okay.',\n",
    "        'Just perfect for me!', 'Very poor quality.', 'Fantastic product!', 'Does not live up to the hype.',\n",
    "        'A great addition to my collection.', 'Extremely bad purchase.', 'Very happy with this.', 'Horrible.',\n",
    "        'I can’t stop using it!', 'Really disappointing.', 'Amazing features and performance!', 'This product is terrible.',\n",
    "        'I absolutely love it!', 'It’s really bad.', 'Totally worth the cost.', 'Poor quality and workmanship.',\n",
    "        'I use it every day!', 'Not as expected.', 'I recommend this to everyone.', 'Extremely overpriced.',\n",
    "        'Best decision I ever made!', 'I am unhappy with the quality.', 'Perfect for my needs!', 'Awful.',\n",
    "        'Highly recommend this product.', 'Wouldn’t buy this again.', 'Incredible product.', 'This is so bad.',\n",
    "        'So glad I bought it!', 'Very low quality.', 'This product works wonderfully!', 'Would not recommend.',\n",
    "        'Great buy!', 'Really bad experience.', 'Love it!', 'The product is defective.'\n",
    "    ],\n",
    "    'sentiment': [\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive',\n",
    "        'positive', 'negative', 'positive', 'negative'\n",
    "    ]\n",
    "}\n",
    "# Preprocess the dataset\n",
    "data['text'] = data['text'][:100]\n",
    "data['sentiment'] = data['sentiment'][:100]\n",
    "\n",
    "# Convert the 'sentiment' column to numeric labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split the dataset into train and test sets (80% train, 20% test)\n",
    "X = df['text']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Preprocess the text using SpaCy\n",
    "def preprocess_text_spacy(text):\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "    words = [token.lemma_ for token in doc if token.text.isalpha()]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train_cleaned = X_train.apply(preprocess_text_spacy)\n",
    "X_test_cleaned = X_test.apply(preprocess_text_spacy)\n",
    "\n",
    "# Step 5: Convert text data into numerical features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_cleaned)\n",
    "X_test_tfidf = vectorizer.transform(X_test_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train multiple models and evaluate their performance\n",
    "# Support Vector Machine (SVM)\n",
    "svm = SVC(kernel='linear', random_state=42)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "svm_pred = svm.predict(X_test_tfidf)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "print('SVM Accuracy:', svm_accuracy)\n",
    "print(classification_report(y_test, svm_pred))\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "log_reg.fit(X_train_tfidf, y_train)\n",
    "log_reg_pred = log_reg.predict(X_test_tfidf)\n",
    "log_reg_accuracy = accuracy_score(y_test, log_reg_pred)\n",
    "print('Logistic Regression Accuracy:', log_reg_accuracy)\n",
    "print(classification_report(y_test, log_reg_pred))\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "rf_pred = rf.predict(X_test_tfidf)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print('Random Forest Accuracy:', rf_accuracy)\n",
    "print(classification_report(y_test, rf_pred))\n",
    "\n",
    "# XGBoost\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train_tfidf, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test_tfidf)\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "print('XGBoost Accuracy:', xgb_accuracy)\n",
    "print(classification_report(y_test, xgb_pred))"
   ]
  }
 ]
}
